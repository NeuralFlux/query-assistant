{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90596ce3-27ce-4bee-b4b5-71910277fcbc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Part 1: Filtering data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d812d6-1d87-4a16-a14f-c9241dded726",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import re\n",
    "\n",
    "import polars as pl\n",
    "from urllib import parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc15787a-e0c1-4c5a-8a98-edf3a4f30beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALLOWED_PARAMS = (\"q\", \"fields\", \"species\")  # need only these to test LLM initially\n",
    "OPS = [\"AND\", \"OR\", \"NOT\", \"(\", \")\", \"?\", \"*\", \"\\d+-\\d+\", '\"', \"~\", \"^\", \"TO\", \"+\"]  # most of ES ops\n",
    "\n",
    "def parse_encoded_uri(encoded_uri):\n",
    "    uri = parse.unquote(encoded_uri)\n",
    "    uri_components = parse.urlparse(uri)\n",
    "\n",
    "    path = uri_components.path.removesuffix(\"/\")\n",
    "    if not path.startswith(\"/v3\"):\n",
    "        path = None\n",
    "\n",
    "    return path, uri_components\n",
    "\n",
    "def filter_uri_params(encoded_uri):\n",
    "    path, uri_components = parse_encoded_uri(encoded_uri)\n",
    "    params = parse.parse_qsl(uri_components.query)\n",
    "\n",
    "    params = filter(lambda x: x[0] in ALLOWED_PARAMS, params)\n",
    "\n",
    "    # hacky use of unquote and quote, can be more efficient by iterating over params\n",
    "    return f\"{path}?{parse.unquote(parse.urlencode(list(params), quote_via=parse.quote))}\"\n",
    "\n",
    "def get_uri_signature(encoded_uri):\n",
    "    path, uri_components = parse_encoded_uri(encoded_uri)\n",
    "    params = parse.parse_qs(uri_components.query) # NOTE: qs vs qsl\n",
    "\n",
    "    # NOTE: we assume len(params[\"param\"]) == 1\n",
    "    # these are just rough filters\n",
    "    q_str = params[\"q\"][0] if \"q\" in params else \"\"\n",
    "    ops_pattern = re.compile(r'(' + '|'.join(map(re.escape, OPS)) + r')')\n",
    "    field_pattern = re.compile(r'([a-zA-Z\\._]+)\\d*:')\n",
    "\n",
    "    q_fields = field_pattern.findall(q_str)\n",
    "    q_ops = ops_pattern.findall(q_str)\n",
    "\n",
    "    fields = params[\"fields\"][0] if \"fields\" in params else \"\"\n",
    "    species = params[\"species\"][0] if \"species\" in params else \"\"\n",
    "    # take only /v3/query or /v3/gene from the path, then sort and join all other fields\n",
    "    sig = (\"/\".join(path.split(\"/\")[:3]), fields, ''.join(sorted(q_ops)), ''.join(sorted(q_fields)), ''.join(sorted(species)))\n",
    "    return tuple(map(lambda s: s.lower(), sig))\n",
    "\n",
    "print(get_uri_signature(\"/v3/gene/1023/?fields=symbol,entrezgene&q=(entrezgene:dipeptidyl%20peptidase%20%28DPP%29-4*)\"))\n",
    "print(get_uri_signature(\"/v3/query?species=human&q=chr3%3A108324182-108324285%3A%20\"))\n",
    "print(get_uri_signature(\"/v3/query/?species=human&fields=symbol&q=Feasibility+of+leadless+left+ventricular+septal+pacing+with+the+WiSE-CRT+system+to+target+the+left+bundle+branch+area:+A+porcine+model+and+multicenter+patient+experience\"))\n",
    "print(filter_uri_params(\"/v3/query/?species=human&fields=symbol&q=Feasibility+of+leadless+left+ventricular+septal+pacing+with+the+WiSE-CRT+system+to+target+the+left+bundle+branch+area:+A+porcine+model+and+multicenter+patient+experience\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb30be2-55c6-4b3d-9033-ee0e8bc7e2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_uri = \"/v3/query/?fields=symbol,entrezgene&q=dipeptidyl%20peptidase%20%28DPP%29-4\"\n",
    "uri = parse.unquote(encoded_uri)\n",
    "uri_components = parse.urlparse(uri)\n",
    "parse.urlencode(parse.parse_qsl(uri_components.query), quote_via=parse.quote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903e0aac-0207-424f-8db3-1774b7124082",
   "metadata": {},
   "outputs": [],
   "source": [
    "parse.urlencode({'fields': 'symbol,entrezgene', 'q': 'dipeptidyl peptidase (DPP)-4'}, quote_via=parse.quote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657778f0-6a84-452a-aa41-c94a4d864e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "path, uri_components = parse_encoded_uri(\"/v3/query?fields=symbol%2Centrezgene&q=1\")\n",
    "parse.parse_qs(uri_components.query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbe97eb-9b7b-4d3c-8a4c-c8762d3dcbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandarallel import pandarallel\n",
    "\n",
    "pandarallel.initialize(progress_bar=True, nb_workers=24)\n",
    "\n",
    "df = pd.read_csv(\"data/mygene_report_last_365_days.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cc6a4b-ec66-4d59-aa71-fd1b6b92989b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90375e0b-de00-4c30-a738-b583d4999c6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# use new.path to construct query_sig\n",
    "# unquote log.path for easier parsing for the downstream LLM\n",
    "df[\"new.path\"] = df[\"log.path\"].parallel_apply(filter_uri_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f168e6ea-68ab-4e16-aaa5-f55aad70c3c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[\"query_sig\"] = df[\"new.path\"].parallel_apply(get_uri_signature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6a1320-37cc-4067-87c3-a511206c8c10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[\"log.path\"] = df[\"log.path\"].parallel_apply(parse.unquote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7419a7c-992c-4b62-97f3-82dceaa6ebd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_df = pl.from_pandas(df)\n",
    "grp = pl_df.group_by(\"query_sig\")\n",
    "filtered = grp.first()  # optionally get longest queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec64ee8-55ca-4eea-b14b-8fbc29120d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make np arrays render on single line in CSV\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "np.set_printoptions(linewidth=np.inf)\n",
    "\n",
    "filtered.to_pandas().to_csv(\"one_year_unique_sig.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35ca814-925d-4b53-8629-b1cbdeeb0923",
   "metadata": {},
   "source": [
    "## Part 2: Generating instruction-query pairs from a bigger LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3bb2646-d94e-494e-9cda-02e36bdaeeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from llama_cpp import Llama, LLAMA_SPLIT_MODE_NONE, LlamaGrammar\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b044e7e5-359c-40a5-be7a-e8ca70ed9950",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 724 tensors from /home/atubati/vendor/weights_llama3.1/Meta-Llama-3.1-70B-Instruct-Q4_K_L.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 70B Instruct\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 70B\n",
      "llama_model_loader: - kv   6:                            general.license str              = llama3.1\n",
      "llama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\n",
      "llama_model_loader: - kv   8:                          general.languages arr[str,8]       = [\"en\", \"de\", \"fr\", \"it\", \"pt\", \"hi\", ...\n",
      "llama_model_loader: - kv   9:                          llama.block_count u32              = 80\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 131072\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 8192\n",
      "llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 28672\n",
      "llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 64\n",
      "llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  17:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models_out/Meta-Llama-3.1-70B-Instru...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 560\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 125\n",
      "llama_model_loader: - type  f32:  162 tensors\n",
      "llama_model_loader: - type q8_0:    2 tensors\n",
      "llama_model_loader: - type q4_K:  440 tensors\n",
      "llama_model_loader: - type q5_K:   40 tensors\n",
      "llama_model_loader: - type q6_K:   80 tensors\n",
      "llm_load_vocab: special tokens cache size = 256\n",
      "llm_load_vocab: token to piece cache size = 0.7999 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 131072\n",
      "llm_load_print_meta: n_embd           = 8192\n",
      "llm_load_print_meta: n_layer          = 80\n",
      "llm_load_print_meta: n_head           = 64\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 8\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 28672\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 131072\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 70B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 70.55 B\n",
      "llm_load_print_meta: model size       = 40.32 GiB (4.91 BPW) \n",
      "llm_load_print_meta: general.name     = Meta Llama 3.1 70B Instruct\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: max token length = 256\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
      "ggml_cuda_init: found 2 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A6000, compute capability 8.6, VMM: yes\n",
      "  Device 1: Tesla V100-PCIE-32GB, compute capability 7.0, VMM: yes\n",
      "llm_load_tensors: ggml ctx size =    1.02 MiB\n",
      "llm_load_tensors: offloading 80 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 81/81 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  1064.62 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size = 23376.89 MiB\n",
      "llm_load_tensors:      CUDA1 buffer size = 16845.29 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 16384\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 500000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =  3072.00 MiB\n",
      "llama_kv_cache_init:      CUDA1 KV buffer size =  2048.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 5120.00 MiB, K (f16): 2560.00 MiB, V (f16): 2560.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model: pipeline parallelism enabled (n_copies=4)\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =  2304.01 MiB\n",
      "llama_new_context_with_model:      CUDA1 compute buffer size =  2304.02 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =   144.02 MiB\n",
      "llama_new_context_with_model: graph nodes  = 2566\n",
      "llama_new_context_with_model: graph splits = 3\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.entries_count': '560', 'quantize.imatrix.dataset': '/training_dir/calibration_datav3.txt', 'quantize.imatrix.chunks_count': '125', 'quantize.imatrix.file': '/models_out/Meta-Llama-3.1-70B-Instruct-GGUF/Meta-Llama-3.1-70B-Instruct.imatrix', 'tokenizer.chat_template': '{{- bos_token }}\\n{%- if custom_tools is defined %}\\n    {%- set tools = custom_tools %}\\n{%- endif %}\\n{%- if not tools_in_user_message is defined %}\\n    {%- set tools_in_user_message = true %}\\n{%- endif %}\\n{%- if not date_string is defined %}\\n    {%- set date_string = \"26 Jul 2024\" %}\\n{%- endif %}\\n\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\n{%- if messages[0][\\'role\\'] == \\'system\\' %}\\n    {%- set system_message = messages[0][\\'content\\']|trim %}\\n    {%- set messages = messages[1:] %}\\n{%- else %}\\n    {%- set system_message = \"\" %}\\n{%- endif %}\\n\\n{#- System message + builtin tools #}\\n{{- \"<|start_header_id|>system<|end_header_id|>\\\\n\\\\n\" }}\\n{%- if builtin_tools is defined or tools is not none %}\\n    {{- \"Environment: ipython\\\\n\" }}\\n{%- endif %}\\n{%- if builtin_tools is defined %}\\n    {{- \"Tools: \" + builtin_tools | reject(\\'equalto\\', \\'code_interpreter\\') | join(\", \") + \"\\\\n\\\\n\"}}\\n{%- endif %}\\n{{- \"Cutting Knowledge Date: December 2023\\\\n\" }}\\n{{- \"Today Date: \" + date_string + \"\\\\n\\\\n\" }}\\n{%- if tools is not none and not tools_in_user_message %}\\n    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\\n    {{- \\'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.\\' }}\\n    {{- \"Do not use variables.\\\\n\\\\n\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \"\\\\n\\\\n\" }}\\n    {%- endfor %}\\n{%- endif %}\\n{{- system_message }}\\n{{- \"<|eot_id|>\" }}\\n\\n{#- Custom tools are passed in a user message with some extra guidance #}\\n{%- if tools_in_user_message and not tools is none %}\\n    {#- Extract the first user message so we can plug it in here #}\\n    {%- if messages | length != 0 %}\\n        {%- set first_user_message = messages[0][\\'content\\']|trim %}\\n        {%- set messages = messages[1:] %}\\n    {%- else %}\\n        {{- raise_exception(\"Cannot put tools in the first user message when there\\'s no first user message!\") }}\\n{%- endif %}\\n    {{- \\'<|start_header_id|>user<|end_header_id|>\\\\n\\\\n\\' -}}\\n    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\\n    {{- \"with its proper arguments that best answers the given prompt.\\\\n\\\\n\" }}\\n    {{- \\'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.\\' }}\\n    {{- \"Do not use variables.\\\\n\\\\n\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \"\\\\n\\\\n\" }}\\n    {%- endfor %}\\n    {{- first_user_message + \"<|eot_id|>\"}}\\n{%- endif %}\\n\\n{%- for message in messages %}\\n    {%- if not (message.role == \\'ipython\\' or message.role == \\'tool\\' or \\'tool_calls\\' in message) %}\\n        {{- \\'<|start_header_id|>\\' + message[\\'role\\'] + \\'<|end_header_id|>\\\\n\\\\n\\'+ message[\\'content\\'] | trim + \\'<|eot_id|>\\' }}\\n    {%- elif \\'tool_calls\\' in message %}\\n        {%- if not message.tool_calls|length == 1 %}\\n            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\\n        {%- endif %}\\n        {%- set tool_call = message.tool_calls[0].function %}\\n        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\\n            {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' -}}\\n            {{- \"<|python_tag|>\" + tool_call.name + \".call(\" }}\\n            {%- for arg_name, arg_val in tool_call.arguments | items %}\\n                {{- arg_name + \\'=\"\\' + arg_val + \\'\"\\' }}\\n                {%- if not loop.last %}\\n                    {{- \", \" }}\\n                {%- endif %}\\n                {%- endfor %}\\n            {{- \")\" }}\\n        {%- else  %}\\n            {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' -}}\\n            {{- \\'{\"name\": \"\\' + tool_call.name + \\'\", \\' }}\\n            {{- \\'\"parameters\": \\' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- \"}\" }}\\n        {%- endif %}\\n        {%- if builtin_tools is defined %}\\n            {#- This means we\\'re in ipython mode #}\\n            {{- \"<|eom_id|>\" }}\\n        {%- else %}\\n            {{- \"<|eot_id|>\" }}\\n        {%- endif %}\\n    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\\n        {{- \"<|start_header_id|>ipython<|end_header_id|>\\\\n\\\\n\" }}\\n        {%- if message.content is mapping or message.content is iterable %}\\n            {{- message.content | tojson }}\\n        {%- else %}\\n            {{- message.content }}\\n        {%- endif %}\\n        {{- \"<|eot_id|>\" }}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' }}\\n{%- endif %}\\n', 'tokenizer.ggml.eos_token_id': '128009', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'llama.rope.dimension_count': '128', 'llama.vocab_size': '128256', 'general.file_type': '15', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '500000.000000', 'general.architecture': 'llama', 'general.basename': 'Meta-Llama-3.1', 'tokenizer.ggml.bos_token_id': '128000', 'llama.attention.head_count': '64', 'tokenizer.ggml.pre': 'llama-bpe', 'llama.context_length': '131072', 'general.name': 'Meta Llama 3.1 70B Instruct', 'general.finetune': 'Instruct', 'general.type': 'model', 'general.size_label': '70B', 'general.license': 'llama3.1', 'llama.feed_forward_length': '28672', 'llama.embedding_length': '8192', 'llama.block_count': '80', 'llama.attention.head_count_kv': '8'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{- bos_token }}\n",
      "{%- if custom_tools is defined %}\n",
      "    {%- set tools = custom_tools %}\n",
      "{%- endif %}\n",
      "{%- if not tools_in_user_message is defined %}\n",
      "    {%- set tools_in_user_message = true %}\n",
      "{%- endif %}\n",
      "{%- if not date_string is defined %}\n",
      "    {%- set date_string = \"26 Jul 2024\" %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- This block extracts the system message, so we can slot it into the right place. #}\n",
      "{%- if messages[0]['role'] == 'system' %}\n",
      "    {%- set system_message = messages[0]['content']|trim %}\n",
      "    {%- set messages = messages[1:] %}\n",
      "{%- else %}\n",
      "    {%- set system_message = \"\" %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- System message + builtin tools #}\n",
      "{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n",
      "{%- if builtin_tools is defined or tools is not none %}\n",
      "    {{- \"Environment: ipython\\n\" }}\n",
      "{%- endif %}\n",
      "{%- if builtin_tools is defined %}\n",
      "    {{- \"Tools: \" + builtin_tools | reject('equalto', 'code_interpreter') | join(\", \") + \"\\n\\n\"}}\n",
      "{%- endif %}\n",
      "{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n",
      "{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n",
      "{%- if tools is not none and not tools_in_user_message %}\n",
      "    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\\n\\n\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\\n\\n\" }}\n",
      "    {%- endfor %}\n",
      "{%- endif %}\n",
      "{{- system_message }}\n",
      "{{- \"<|eot_id|>\" }}\n",
      "\n",
      "{#- Custom tools are passed in a user message with some extra guidance #}\n",
      "{%- if tools_in_user_message and not tools is none %}\n",
      "    {#- Extract the first user message so we can plug it in here #}\n",
      "    {%- if messages | length != 0 %}\n",
      "        {%- set first_user_message = messages[0]['content']|trim %}\n",
      "        {%- set messages = messages[1:] %}\n",
      "    {%- else %}\n",
      "        {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n",
      "{%- endif %}\n",
      "    {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n",
      "    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n",
      "    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\\n\\n\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\\n\\n\" }}\n",
      "    {%- endfor %}\n",
      "    {{- first_user_message + \"<|eot_id|>\"}}\n",
      "{%- endif %}\n",
      "\n",
      "{%- for message in messages %}\n",
      "    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n",
      "        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' }}\n",
      "    {%- elif 'tool_calls' in message %}\n",
      "        {%- if not message.tool_calls|length == 1 %}\n",
      "            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n",
      "        {%- endif %}\n",
      "        {%- set tool_call = message.tool_calls[0].function %}\n",
      "        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\n",
      "            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
      "            {{- \"<|python_tag|>\" + tool_call.name + \".call(\" }}\n",
      "            {%- for arg_name, arg_val in tool_call.arguments | items %}\n",
      "                {{- arg_name + '=\"' + arg_val + '\"' }}\n",
      "                {%- if not loop.last %}\n",
      "                    {{- \", \" }}\n",
      "                {%- endif %}\n",
      "                {%- endfor %}\n",
      "            {{- \")\" }}\n",
      "        {%- else  %}\n",
      "            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
      "            {{- '{\"name\": \"' + tool_call.name + '\", ' }}\n",
      "            {{- '\"parameters\": ' }}\n",
      "            {{- tool_call.arguments | tojson }}\n",
      "            {{- \"}\" }}\n",
      "        {%- endif %}\n",
      "        {%- if builtin_tools is defined %}\n",
      "            {#- This means we're in ipython mode #}\n",
      "            {{- \"<|eom_id|>\" }}\n",
      "        {%- else %}\n",
      "            {{- \"<|eot_id|>\" }}\n",
      "        {%- endif %}\n",
      "    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n",
      "        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n",
      "        {%- if message.content is mapping or message.content is iterable %}\n",
      "            {{- message.content | tojson }}\n",
      "        {%- else %}\n",
      "            {{- message.content }}\n",
      "        {%- endif %}\n",
      "        {{- \"<|eot_id|>\" }}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n",
      "{%- endif %}\n",
      "\n",
      "Using chat eos_token: <|eot_id|>\n",
      "Using chat bos_token: <|begin_of_text|>\n"
     ]
    }
   ],
   "source": [
    "llm = Llama(\n",
    "    model_path=os.path.expanduser(\"~/vendor/weights_llama3.1/Meta-Llama-3.1-70B-Instruct-Q4_K_L.gguf\"),\n",
    "    n_gpu_layers=-1, # Uncomment to use GPU acceleration\n",
    "    # seed=1337, # Uncomment to set a specific seed\n",
    "    n_ctx=16384, # Uncomment to increase the context window\n",
    "    # split_mode=LLAMA_SPLIT_MODE_NONE  # Uncomment to use single-GPU\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cc0e91b-7edf-404a-8178-7047bb03b1c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data = pd.read_csv(\"data/one_year_unique_sig.csv\")[\"log.path\"].values\n",
    "data = pd.read_csv(\"data/sample_logs.csv\")[\"log.path\"].values\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "873af593-d269-4ee7-ab63-7fe9cab041a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/prompts/datagen_prompt.md\") as fd:\n",
    "    base_prompt = fd.read()\n",
    "\n",
    "with open(\"gene_query_docs.txt\") as doc_fd:\n",
    "    docs = doc_fd.read()\n",
    "\n",
    "with open(\"data/original/compact_desc.csv\") as desc_fd:\n",
    "    description = desc_fd.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f70c1f3-d4f0-4ee4-b7e0-77ca87ccfe9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_schema = {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"instructions\": {\n",
    "                            \"type\": \"array\",\n",
    "                            \"items\": {\n",
    "                                \"type\": \"object\",\n",
    "                                \"properties\": {\n",
    "                                    \"query\": {\"type\": \"string\"},\n",
    "                                    \"imperative\": {\"type\": \"string\"},\n",
    "                                    \"question\": {\"type\": \"string\"}\n",
    "                                },\n",
    "                                \"required\": [\"query\", \"imperative\", \"question\"]\n",
    "                            }\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"instructions\"]\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6196154a-f16f-4416-acbb-b04258dc1be1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You need to generate instructions for API queries and the instruction-query pairs will be used to fine-tune a smaller LLM assistant. Generate two instructions for each of the 10 given queries, one imperative and one question-style.\n",
      "\n",
      "Here are the requirements:\n",
      "1. Strictly output a JSON.\n",
      "2. Use the documentation and summary of fields in the database as reference.\n",
      "3. The instructions must be in English.\n",
      "4. Use diverse verbs in your responses to generalize better. This is very important!\n",
      "5. A small language model should be able to complete the instruction. For example, do not ask the assistant to create any visual or audio output. For another example, do not ask the assistant to wake you up at 5pm or set a reminder because it cannot perform any action.\n",
      "6. Each instruction must be 3 sentences long at the most.\n",
      "\n",
      "Here are the 10 API queries line-wise:\n",
      "/v3/query?q=ensemblgene:7087\tICAM5\t19\t19p13.2\tprotein-coding\tTLCN\tHGNC:5348\tENSG00000105376\n",
      "/v3/query?q=symbol:CDK1+AND+taxid:9606&fields=go.BP&species=human&size=1\n",
      "/v3/query?q=p53&species=human&fields=symbol,name,entrezgene,pantherdb&size=1\n",
      "/v3/query?q=CAND1&fields=symbol,genecards&species=human\n",
      "/v3/query?species=7955&fields=symbol,ensembl.gene&q=symbol:ensda*\n",
      "/v3/gene/ENSG00000165629?species=human&fields=ensembl&dotfield=false&size=10\n",
      "/v3/gene/1017?fields=symbol,name,entrezgene,refseq\n",
      "/v3/gene/100128908?fields=name,symbol,refseq.rna\n",
      "/v3/gene/4157?species=human&fields=ensembl.gene=all&dotfield=true&size=10\n",
      "/v3/query?q=apoptosis&species=human&fields=entrezgene,go.BP&size=100\n",
      "char ::= [^\"\\] | [\\] char_1 \n",
      "char_1 ::= [\"\\/bfnrt] | [u] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] \n",
      "instructions ::= [[] space instructions_8 []] space \n",
      "space ::= space_15 \n",
      "instructions_4 ::= instructions-item instructions_7 \n",
      "instructions-item ::= [{] space instructions-item-query-kv [,] space instructions-item-imperative-kv [,] space instructions-item-question-kv [}] space \n",
      "instructions_6 ::= [,] space instructions-item \n",
      "instructions_7 ::= instructions_6 instructions_7 | \n",
      "instructions_8 ::= instructions_4 | \n",
      "instructions-item-query-kv ::= [\"] [q] [u] [e] [r] [y] [\"] space [:] space string \n",
      "instructions-item-imperative-kv ::= [\"] [i] [m] [p] [e] [r] [a] [t] [i] [v] [e] [\"] space [:] space string \n",
      "instructions-item-question-kv ::= [\"] [q] [u] [e] [s] [t] [i] [o] [n] [\"] space [:] space string \n",
      "string ::= [\"] string_16 [\"] space \n",
      "instructions-kv ::= [\"] [i] [n] [s] [t] [r] [u] [c] [t] [i] [o] [n] [s] [\"] space [:] space instructions \n",
      "root ::= [{] space instructions-kv [}] space \n",
      "space_15 ::= [ ] | \n",
      "string_16 ::= char string_16 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    2602.08 ms\n",
      "llama_print_timings:      sample time =   15006.67 ms /   939 runs   (   15.98 ms per token,    62.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =   31112.94 ms /  8933 tokens (    3.48 ms per token,   287.12 tokens per second)\n",
      "llama_print_timings:        eval time =   84759.76 ms /   938 runs   (   90.36 ms per token,    11.07 tokens per second)\n",
      "llama_print_timings:       total time =  135948.72 ms /  9871 tokens\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 10  # NOTE: change in prompt also if changed\n",
    "for idx in range(0, len(data), BATCH_SIZE):\n",
    "    prompt = f\"{base_prompt}\\n\" + \"\\n\".join(data[idx:(idx + BATCH_SIZE)])\n",
    "    print(prompt)\n",
    "    output = llm.create_chat_completion(\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "            {\"role\": \"system\", \"content\": f\"Use this documentation for help: {docs}\\n\\nand this Schema: {description}\"},\n",
    "        ],\n",
    "        # grammar=query_grammar\n",
    "        response_format={\n",
    "            \"type\": \"json_object\",\n",
    "            \"schema\": output_schema\n",
    "        },\n",
    "    )\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d563d3ae-b124-43a2-a468-e77179dd5640",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "resp = json.loads(output[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b575da6f-ab8e-415a-90b0-8c2407995913",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(resp[\"instructions\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5f701ad-a6bc-4c53-a3ec-e58251bd6521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'query': '/v3/query?q=ensemblgene:7087\\tICAM5\\t19\\t19p13.2\\tprotein-coding\\tTLCN\\tHGNC:5348\\tENSG00000105376',\n",
       "  'imperative': 'Find the gene with Ensembl gene ID 7087 and return its symbol, name, chromosome, genomic position, type of gene, alias, HGNC ID, and Ensembl gene ID.',\n",
       "  'question': 'What are the symbol, name, chromosome, genomic position, type of gene, alias, HGNC ID, and Ensembl gene ID of the gene with Ensembl gene ID 7087?'},\n",
       " {'query': '/v3/query?q=symbol:CDK1+AND+taxid:9606&fields=go.BP&species=human&size=1',\n",
       "  'imperative': 'Find the human gene with symbol CDK1 and return its Gene Ontology Biological Process information.',\n",
       "  'question': 'What is the Gene Ontology Biological Process information of the human gene with symbol CDK1?'},\n",
       " {'query': '/v3/query?q=p53&species=human&fields=symbol,name,entrezgene,pantherdb&size=1',\n",
       "  'imperative': 'Find the human gene with symbol p53 and return its symbol, name, Entrez gene ID, and PANTHER database information.',\n",
       "  'question': 'What are the symbol, name, Entrez gene ID, and PANTHER database information of the human gene with symbol p53?'},\n",
       " {'query': '/v3/query?q=CAND1&fields=symbol,genecards&species=human',\n",
       "  'imperative': 'Find the human gene with symbol CAND1 and return its symbol and GeneCards information.',\n",
       "  'question': 'What are the symbol and GeneCards information of the human gene with symbol CAND1?'},\n",
       " {'query': '/v3/query?species=7955&fields=symbol,ensembl.gene&q=symbol:ensda*',\n",
       "  'imperative': 'Find the genes in species 7955 with symbol starting with ensda and return their symbol and Ensembl gene ID.',\n",
       "  'question': 'What are the symbol and Ensembl gene ID of the genes in species 7955 with symbol starting with ensda?'},\n",
       " {'query': '/v3/gene/ENSG00000165629?species=human&fields=ensembl&dotfield=false&size=10',\n",
       "  'imperative': 'Find the human gene with Ensembl gene ID ENSG00000165629 and return its Ensembl information in a nested format.',\n",
       "  'question': 'What is the Ensembl information of the human gene with Ensembl gene ID ENSG00000165629?'},\n",
       " {'query': '/v3/gene/1017?fields=symbol,name,entrezgene,refseq',\n",
       "  'imperative': 'Find the gene with Entrez gene ID 1017 and return its symbol, name, Entrez gene ID, and RefSeq information.',\n",
       "  'question': 'What are the symbol, name, Entrez gene ID, and RefSeq information of the gene with Entrez gene ID 1017?'},\n",
       " {'query': '/v3/gene/100128908?fields=name,symbol,refseq.rna',\n",
       "  'imperative': 'Find the gene with Entrez gene ID 100128908 and return its name, symbol, and RefSeq RNA information.',\n",
       "  'question': 'What are the name, symbol, and RefSeq RNA information of the gene with Entrez gene ID 100128908?'},\n",
       " {'query': '/v3/gene/4157?species=human&fields=ensembl.gene=all&dotfield=true&size=10',\n",
       "  'imperative': 'Find the human gene with Entrez gene ID 4157 and return all its Ensembl gene information in a flattened format.',\n",
       "  'question': 'What is all the Ensembl gene information of the human gene with Entrez gene ID 4157?'},\n",
       " {'query': '/v3/query?q=apoptosis&species=human&fields=entrezgene,go.BP&size=100',\n",
       "  'imperative': 'Find the human genes associated with apoptosis and return their Entrez gene ID and Gene Ontology Biological Process information.',\n",
       "  'question': 'What are the Entrez gene ID and Gene Ontology Biological Process information of the human genes associated with apoptosis?'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp[\"instructions\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb32f51-9c88-49c4-8c7e-4e21a3f73014",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(resp[\"instructions\"]).to_csv(\"data/ft/sample_pairs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0107c132-fd4c-4c9e-9f52-7dc056d8a9f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
